{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Identifying characteristic scales of the processes of interest is a crucial requirement towards establishing statistical inference between the spatial patterns of landscapes and the process that operate upon them [1, 2]. Nonetheless, spatial interactions can occur across multiple scales and in practice characteristic scales may not be easy to identify. Therefore a common approach is to consider multiple spatial scales.\n",
    "\n",
    "The central idea of multilandpy is to extend this landscape ecology idea beyond landscape metrics (e.g., see the [mulilandR package](https://github.com/phuais/multilandR) [3] or [the multi-scale analysis draft of pylandstats](https://github.com/martibosch/pylandstats-notebooks/blob/main/notebooks/06-multiscale-analysis.ipynb) [4]) andfacilitate computing mutli-scale landscape features in Python. As an example use case, we will explore the multi-scale interactions between temperature measurements from official and citizen weather stations (CWS) and the spatial pattern of an urban landscape in Zurich, Switzerland. To that end, we will use data from swisstopo to compute the amount of buildings (in footprint area and volume), tree canopy proportion and several topographic features. Each feature will be computed at multiple scales for each meteorological station, i.e., in the surrounding urban landscape for multiple buffer distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as lg\n",
    "import os\n",
    "from collections.abc import Callable, Mapping\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import multilandpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import pooch\n",
    "import seaborn as sns\n",
    "from meteora.clients import GHCNHourlyClient, NetatmoClient\n",
    "from multilandpy import settings\n",
    "from shapely import geometry\n",
    "from sklearn import decomposition, linear_model, model_selection, preprocessing\n",
    "\n",
    "# disable pooch logs\n",
    "logger = pooch.get_logger()\n",
    "logger.setLevel(\"WARNING\")\n",
    "\n",
    "# enable multiurbanpy logs\n",
    "lg.basicConfig(level=lg.INFO)\n",
    "settings.LOG_CONSOLE = True\n",
    "\n",
    "\n",
    "def get_X_df(\n",
    "    features_df: pd.DataFrame,\n",
    "    *,\n",
    "    scaler_class: Callable | bool | None = None,\n",
    "    **scaler_kwargs: Mapping,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get multi-scale features for analysis with scikit-learn.\n",
    "\n",
    "    Returns a flat (one row per grid cell/sample) data frame, which keeps only one\n",
    "    buffer distance for the elevation feature (to avoid artificially duplicating\n",
    "    features and inflating colinearity). Optionally use a scikit-learn preprocessing\n",
    "    class to scale, center, normalize (or the like) the data (by default, use normal\n",
    "    Gaussian standardization).\n",
    "    \"\"\"\n",
    "    flat_grid_df = features_df.unstack(\"buffer_dist\").drop(\n",
    "        [(\"elevation\", buffer_dist) for buffer_dist in buffer_dists[1:]], axis=\"columns\"\n",
    "    )\n",
    "    if scaler_class is None:\n",
    "        scaler_class = preprocessing.StandardScaler\n",
    "    if scaler_class:\n",
    "        X = scaler_class(**scaler_kwargs).fit_transform(flat_grid_df)\n",
    "    else:\n",
    "        X = flat_grid_df\n",
    "    return pd.DataFrame(\n",
    "        X,\n",
    "        index=flat_grid_df.index,\n",
    "        columns=flat_grid_df.columns,\n",
    "    )\n",
    "\n",
    "\n",
    "def aic(\n",
    "    model: Callable,\n",
    "    X_train: npt.ArrayLike,\n",
    "    y_train: npt.ArrayLike,\n",
    "    X_test: npt.ArrayLike,\n",
    "    y_test: npt.ArrayLike,\n",
    "    **fit_kwargs: Mapping,\n",
    ") -> float:\n",
    "    \"\"\"Compute Akaike Information Criterion (AIC) for the test set.\"\"\"\n",
    "    return 2 * X_train.shape[1] - 2 * np.log(\n",
    "        np.sum(\n",
    "            (y_test - model().fit(X_train, y_train, **fit_kwargs).predict(X_test)) ** 2,\n",
    "            axis=0,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "region = [8.46617, 47.34707, 8.57655, 47.44401]\n",
    "crs = \"epsg:2056\"\n",
    "\n",
    "# Netatmo credentials\n",
    "client_id = os.getenv(\"NETATMO_CLIENT_ID\", default=\"\")\n",
    "client_secret = os.getenv(\"NETATMO_CLIENT_SECRET\", default=\"\")\n",
    "scale = \"1hour\"  # default is \"30min\"\n",
    "\n",
    "# to compute features\n",
    "# download data from zenodo\n",
    "_zenodo_base_url = \"https://zenodo.org/records/15166651/files\"\n",
    "# ACHTUNG: computing topographic features requires a windowed reading that does not work\n",
    "# if we stream the geotiff directly from zenodo, so it is better to download files\n",
    "# locally\n",
    "buildings_url = f\"{_zenodo_base_url}/buildings.gpkg?download=1\"\n",
    "dem_url = f\"{_zenodo_base_url}/dem.tif?download=1\"\n",
    "tree_canopy_url = f\"{_zenodo_base_url}/tree-canopy.tif?download=1\"\n",
    "buildings_filepath = pooch.retrieve(\n",
    "    url=buildings_url,\n",
    "    known_hash=None,\n",
    ")\n",
    "dem_filepath = pooch.retrieve(\n",
    "    url=dem_url,\n",
    "    known_hash=None,\n",
    ")\n",
    "tree_canopy_filepath = pooch.retrieve(\n",
    "    url=tree_canopy_url,\n",
    "    known_hash=None,\n",
    ")\n",
    "tree_val = 1  # value that represents a tree in the canopy raster\n",
    "buffer_dists = [25, 50, 100, 250, 500]\n",
    "grid_res = 200\n",
    "\n",
    "# for PCA\n",
    "target_evr = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Getting example data\n",
    "\n",
    "As introduced above, multilandpy can be used any analysis case in which we have a set of site locations (can be any site where data is collected, e.g., environmental sensors, cameras or the like) for which we want to compute landscape features (vegetation, topography, buildings...) at multiple spatial scales (buffers of increasing radii around each site). Additionally, we are usually interested in linking the computed multi-scale spatial features with an environmental, socioeconomic or any kind of process that occurs upon the landscape.\n",
    "\n",
    "Accordingly, as an example use case, we will explore the spatial patterns that surround meteorological stations and assess how they relate to the observed temperature, e.g., through the urban heat island (UHI) effect. We will use [meteora](https://github.com/martibosch/meteora) to retrieve data from official weather stations from the [Global Historical Climatology Network hourly (GHCNh)](https://www.ncei.noaa.gov/products/global-historical-climatology-network-hourly) and [CWS from Netatmo](https://weathermap.netatmo.com). Before querying actual measurements, let us start by focusing on the available station locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnh_client = GHCNHourlyClient(region)\n",
    "netatmo_client = NetatmoClient(\n",
    "    region,\n",
    "    client_id,\n",
    "    client_secret,\n",
    ")\n",
    "stations_gdf = pd.concat(\n",
    "    [\n",
    "        gdf.assign(source=source)\n",
    "        for source, gdf in zip(\n",
    "            [\"MeteoSwiss\", \"CWS\"],\n",
    "            [ghcnh_client.stations_gdf, netatmo_client.stations_gdf],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "ax = stations_gdf.plot(\"source\", legend=True)\n",
    "cx.add_basemap(ax, crs=stations_gdf.crs, attribution=False)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "*(C) OpenStreetMap contributors, Tiles style by Humanitarian OpenStreetMap Team hosted by OpenStreetMap France*\n",
    "\n",
    "### Computing multi-scale landscape features\n",
    "\n",
    "As we can see, the spatial density of CWS from Netatmo is much higher than that of official stations from the GHCNh network. We will now compute, at multiple scales around each station, the building areas and volumes, proportion of tree canopy and topographic features, i.e., slope, northness and topographic position index (TPI). To that end, we first need the following datasets:\n",
    "\n",
    "- a **building footprints** geo-data frame, ideally with height information to compute building volumes (otherwise, only area-based features are computed). This is actually an optional requirement - if not provided, the building footprints for the given region will be automatically retrieved (without heihgt information) from the [OpenStreetMap](https://openstreetmap.org) using [osmnx](https://github.com/gboeing/osmnx).\n",
    "- a **tree canopy** raster\n",
    "- a **digital elevation model (DEM)** raster\n",
    "\n",
    "Luckily, since our study area is in Switzerland, we can easily obtain the above datasets from [the swisstopo STAC API](https://www.geo.admin.ch/en/rest-interface-stac-api) using [swisstopopy](https://github.com/martibosch/swisstopopy) (see [doi.org/10.5281/zenodo.15166651](https://doi.org/10.5281/zenodo.15166651)). Given the required data, the multi-scale features can be computed using the `MultiScaleFeatureComputer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# reproject the region first\n",
    "reproj_region = gpd.GeoSeries([geometry.box(*region)], crs=\"epsg:4326\").to_crs(crs)\n",
    "# get station locations as a geo-series\n",
    "site_gser = stations_gdf[\"geometry\"].to_crs(crs)\n",
    "\n",
    "# initialize the mutli-scale feature computer for our (reprojected) region\n",
    "msfc = multilandpy.MultiScaleFeatureComputer(\n",
    "    region=reproj_region,\n",
    "    crs=crs,\n",
    ")\n",
    "# compute multi-scale features\n",
    "building_features_df = msfc.compute_building_features(\n",
    "    site_gser, buffer_dists, building_gdf=buildings_filepath\n",
    ")\n",
    "tree_features_df = msfc.compute_tree_features(\n",
    "    tree_canopy_filepath, site_gser, buffer_dists, tree_val\n",
    ")\n",
    "topo_features_df = msfc.compute_topo_features_df(dem_filepath, site_gser, buffer_dists)\n",
    "# compute the elevation at the station location (independent of the buffer radii):\n",
    "elevation_ser = msfc.compute_elevation_ser(dem_filepath, site_gser)\n",
    "\n",
    "# assemble into a single data frame\n",
    "station_features_df = pd.concat(\n",
    "    [building_features_df, tree_features_df, topo_features_df], axis=\"columns\"\n",
    ").fillna(0)\n",
    "# set elevation (even though is not multi-scale)\n",
    "station_features_df[\"elevation\"] = station_features_df.index.get_level_values(\n",
    "    \"station_id\"\n",
    ").map(elevation_ser)\n",
    "# station_features_df.to_csv(\"data/station-features.csv\")\n",
    "station_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Maximizing site variation\n",
    "\n",
    "In order to improve the statistical significance of the relationships between spatial patterns and the processes of interests, the selected sites and scale should cover the maximum possible range of landscape heterogeneity to maximize the variance of independent variables [2]. Let us use a regular grid to explore the potential heterogeneity of our area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gser = msfc.generate_regular_grid_gser(grid_res, geometry_type=\"point\")\n",
    "grid_features_df = pd.concat(\n",
    "    [\n",
    "        msfc.compute_building_features(\n",
    "            grid_gser, buffer_dists, building_gdf=buildings_filepath\n",
    "        ),\n",
    "        msfc.compute_tree_features(\n",
    "            tree_canopy_filepath, grid_gser, buffer_dists, tree_val\n",
    "        ),\n",
    "        msfc.compute_topo_features_df(dem_filepath, grid_gser, buffer_dists),\n",
    "    ],\n",
    "    axis=\"columns\",\n",
    ").fillna(0)\n",
    "elevation_ser = msfc.compute_elevation_ser(dem_filepath, grid_gser)\n",
    "# set elevation (even though is not multi-scale)\n",
    "grid_features_df[\"elevation\"] = grid_features_df.index.get_level_values(\n",
    "    grid_gser.index.name\n",
    ").map(elevation_ser)\n",
    "grid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat(\n",
    "    [\n",
    "        gpd.GeoDataFrame(geometry=grid_gser).assign(source=\"Grid\"),\n",
    "        stations_gdf.to_crs(grid_gser.crs),\n",
    "    ]\n",
    ").plot(\"source\", alpha=0.5, legend=True)\n",
    "cx.add_basemap(ax, crs=grid_gser.crs, attribution=False)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We will now perform a principal component analysis (PCA) over the grid's computed features to explore their variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a flat (one row per grid cell/sample) data frame in line with sklearn\n",
    "# also keep only one buffer distance for the elevation feature (to avoid artificially\n",
    "# duplicating features and inflating colinearity)\n",
    "X_grid_df = get_X_df(grid_features_df)\n",
    "\n",
    "for n_components in range(1, len(X_grid_df.columns)):\n",
    "    pca = decomposition.PCA(n_components=n_components).fit(X_grid_df)\n",
    "    evr = pca.explained_variance_ratio_.sum()\n",
    "    if evr >= target_evr:\n",
    "        break\n",
    "X_grid_pca = pd.DataFrame(pca.transform(X_grid_df), index=X_grid_df.index)\n",
    "n_components, pca.explained_variance_ratio_[:2].sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "As we can se, we need 7 components to reach the target explained variance ratio (EVR) of 90%, and that the first two components explain a 53.24 % of the total EVR. Note that we have a total of 31 features, namely 6 features at 5 scales plus the elevation. Let us now see the component loadings on each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.DataFrame(\n",
    "    pca.components_.T, columns=X_grid_pca.columns, index=X_grid_df.columns\n",
    ")\n",
    "v = load_df.abs().max().max()\n",
    "sns.heatmap(load_df, cmap=\"coolwarm\", vmin=-v, vmax=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can visualize a scatterplot of each grid cell in the first two components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    X_grid_pca,\n",
    "    x=0,\n",
    "    y=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "or using the two variables that have the highest loadings on the first and second component respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = (load_df.index[load_df[i].abs().argmax()] for i in range(2))\n",
    "ax = sns.scatterplot(\n",
    "    X_grid_df,\n",
    "    x=x,\n",
    "    y=y,\n",
    ")\n",
    "ax.set_xlabel(f\"{x[0]}-{x[1]}\")\n",
    "ax.set_ylabel(f\"{y[0]}-{y[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "This gives us an idea of the variance of our landscape. We can now asses how well the existing stations cover such feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stations_df = get_X_df(station_features_df.fillna(0))\n",
    "\n",
    "X_stations_pca = pd.DataFrame(pca.transform(X_stations_df), index=X_stations_df.index)\n",
    "\n",
    "sns.scatterplot(\n",
    "    pd.concat(\n",
    "        [\n",
    "            X_grid_pca.assign(source=\"grid\"),\n",
    "            X_stations_pca.assign(\n",
    "                source=X_stations_pca.index.map(stations_gdf[\"source\"])\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ),\n",
    "    x=0,\n",
    "    y=1,\n",
    "    hue=\"source\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "or again, visualize a scatterplot using each component's variable with the highest loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = (load_df.index[load_df[i].abs().argmax()] for i in range(2))\n",
    "ax = sns.scatterplot(\n",
    "    pd.concat(\n",
    "        [\n",
    "            X_grid_df.assign(source=\"grid\"),\n",
    "            X_stations_df.assign(\n",
    "                source=X_stations_df.index.map(stations_gdf[\"source\"])\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ),\n",
    "    x=x,\n",
    "    y=y,\n",
    "    hue=\"source\",\n",
    ")\n",
    "ax.set_xlabel(f\"{x[0]}-{x[1]}\")\n",
    "ax.set_ylabel(f\"{y[0]}-{y[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "\n",
    "Such visualizations illustrate how well the official stations and CWS cover the variance of our landscape. For instance, we can see that unlike CWS (Netatmo), official (GHCNh) stations only cover locations that are not too surrounded of building volume, which suggests that models based on official stations solely may not be able to provide accurate predictions under such circumstances.\n",
    "\n",
    "In clear analogy with site selection in landscape ecology, e.g., to collect biodiversity (or other field measurements of a target ecological response) and study the effects of the surrounding spatial patterns [2, 5], we can use this information to select new locations to set up new meteorological stations.\n",
    "\n",
    "### Scale of effect\n",
    "\n",
    "The *scale of effect* is the scale at which the relationship between the ecological process and the landscape spatial patterns show its strongest effect [1]. It is possible to statistically estimate the *scale of effect*, e.g., by exploring at which scale the selected model exhibits the strongest determination [3]:\n",
    "\n",
    "Let us now get a time series of data during a heatwave in August 2023, defined as at least 3 consecutive days with an average temperature over 25$^{\\circ}$C ([based on the heat level warning definitions by MeteoSwiss](https://www.meteoswiss.admin.ch/weather/weather-and-climate-from-a-to-z/heat-warnings.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "variables = [\"temperature\"]\n",
    "start = \"18-08-2023\"\n",
    "end = \"25-08-2023\"\n",
    "y = pd.concat(\n",
    "    [\n",
    "        ts_df.groupby(\"station_id\").mean()\n",
    "        for ts_df in [\n",
    "            ghcnh_client.get_ts_df(variables, start, end),\n",
    "            netatmo_client.get_ts_df(variables, start, end, scale=scale),\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We can now train a model to predict the average station temperature based on the multi-scale features and use a statistical determination measure, e.g., the Akaike information criterion (AIC), to estimate the *scale of effect*. Since this notebook is mostly a conceptual draft, let us use a simple model, i.e., linear regression and train separate instances for each scale. We will use 50 repetitions in which we shuffle the samples and then select half of them for training and the other half for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repetitions = 50\n",
    "test_size = 0.5\n",
    "\n",
    "df = station_features_df.loc[y.index]\n",
    "X_df = pd.DataFrame(preprocessing.StandardScaler().fit_transform(df), index=df.index)\n",
    "\n",
    "# model\n",
    "model = linear_model.LinearRegression\n",
    "\n",
    "results = []\n",
    "for buffer_dist, df in X_df.groupby(level=\"buffer_dist\"):\n",
    "    # drop nan\n",
    "    df = df.dropna()\n",
    "    _y = y.loc[df.index.get_level_values(\"station_id\")]\n",
    "    for i in range(n_repetitions):\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "            df, _y, test_size=test_size, shuffle=True\n",
    "        )\n",
    "        results.append(\n",
    "            (buffer_dist, i, aic(model, X_train, y_train, X_test, y_test).item())\n",
    "        )\n",
    "eval_df = pd.DataFrame(results, columns=[\"Buffer dist.\", \"repetition\", \"AIC\"])\n",
    "sns.lineplot(eval_df, x=\"Buffer dist.\", y=\"AIC\", marker=\"o\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "This suggests that the *scale of effect* is around 100 m, which is in line with the building volume at the 100 m scale having the highest loading on the first PCA component.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Jackson, H. B., & Fahrig, L. (2015). Are ecologists conducting research at the optimal scale?. Global Ecology and Biogeography, 24(1), 52-63.\n",
    "2. Hesselbarth, M. H., Nowosad, J., de Flamingh, A., Simpkins, C. E., Jung, M., Gerber, G., & Bosch, M. (2025). Computational Methods in Landscape Ecology. Current Landscape Ecology Reports, 10(1), 1-18.\n",
    "3. Huais, P. Y. (2024). Multilandr: An r package for multi-scale landscape analysis. Landscape Ecology, 39(8), 140.\n",
    "4. Bosch, M. (2019). PyLandStats: An open-source Pythonic library to compute landscape metrics. PloS one, 14(12), e0225734.\n",
    "5. Pasher, J., Mitchell, S. W., King, D. J., Fahrig, L., Smith, A. C., & Lindsay, K. E. (2013). Optimizing landscape selection for estimating relative effects of landscape variables on ecological responses. Landscape ecology, 28, 371-383."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all"
  },
  "kernelspec": {
   "display_name": "Python (multilandpy)",
   "language": "python",
   "name": "multilandpy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
